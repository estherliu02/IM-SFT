_wandb:
    value:
        cli_version: 0.21.0
        e:
            c2puw32ickkbt9cd3udn6id8jabl4nco:
                args:
                    - --model_name_or_path
                    - meta-llama/Llama-3.1-8B-Instruct
                    - --tokenizer_name
                    - meta-llama/Llama-3.1-8B-Instruct
                    - --use_flash_attn
                    - --train_file
                    - data/silverpairs_prompt_completion.jsonl
                    - --max_seq_length
                    - "2048"
                    - --preprocessing_num_workers
                    - "16"
                    - --per_device_train_batch_size
                    - "1"
                    - --gradient_accumulation_steps
                    - "8"
                    - --learning_rate
                    - "1e-4"
                    - --lr_scheduler_type
                    - cosine
                    - --warmup_ratio
                    - "0.05"
                    - --weight_decay
                    - "0.0"
                    - --checkpointing_steps
                    - "50"
                    - --num_train_epochs
                    - "6"
                    - --output_dir
                    - output/llama3-8b_im_sft
                    - --use_lora
                    - --lora_rank
                    - "8"
                    - --lora_alpha
                    - "32"
                    - --lora_dropout
                    - "0.05"
                    - --lora_target
                    - q_proj,v_proj
                    - --with_tracking
                    - --report_to
                    - wandb
                    - --logging_steps
                    - "1"
                    - --use_lm_loss
                    - --rope_scaling_type
                    - linear
                    - --rope_scaling_factor
                    - "2.0"
                    - --run_name
                    - llama3-8b-im
                codePath: src/finetune.py
                codePathLocal: src/finetune.py
                cpu_count: 64
                cpu_count_logical: 128
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "107374182400"
                        used: "52171796480"
                email: gefei021021@gmail.com
                executable: /venv/sft/bin/python3.10
                git:
                    commit: 4139918dc4a70e2ba9bcc000e41ea4e620cce0a2
                    remote: https://github.com/estherliu02/IM-SFT.git
                gpu: NVIDIA A100-SXM4-80GB
                gpu_count: 2
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 6912
                      memoryTotal: "85899345920"
                      name: NVIDIA A100-SXM4-80GB
                      uuid: GPU-c3c887dc-b3aa-ec16-48a0-97c8a207982e
                    - architecture: Ampere
                      cudaCores: 6912
                      memoryTotal: "85899345920"
                      name: NVIDIA A100-SXM4-80GB
                      uuid: GPU-24e7b625-fc0b-445e-aa33-6bf454154fbf
                host: 4f7eefb40635
                memory:
                    total: "540590526464"
                os: Linux-6.8.0-51-generic-x86_64-with-glibc2.35
                program: /workspace/IM-SFT/src/finetune.py
                python: CPython 3.10.18
                root: /workspace/IM-SFT
                startedAt: "2025-07-24T00:39:54.052698Z"
                writerId: c2puw32ickkbt9cd3udn6id8jabl4nco
        m: []
        python_version: 3.10.18
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 98
                - 105
            "2":
                - 1
                - 11
                - 49
                - 51
                - 71
                - 98
                - 105
            "3":
                - 2
                - 13
                - 61
            "4": 3.10.18
            "5": 0.21.0
            "6": 4.35.0.dev0
            "12": 0.21.0
            "13": linux-x86_64
apply_kl_div_loss:
    value: false
block_size:
    value: null
checkpointing_steps:
    value: "50"
clip_grad_norm:
    value: -1
config_name:
    value: null
dataset_config_name:
    value: null
dataset_name:
    value: null
gradient_accumulation_steps:
    value: 8
gradient_checkpointing:
    value: false
kl_penalty_ctl:
    value: 1
learning_rate:
    value: 0.0001
logging_steps:
    value: 1
lora_alpha:
    value: 32
lora_dropout:
    value: 0.05
lora_rank:
    value: 8
lora_target:
    value: q_proj,v_proj
low_cpu_mem_usage:
    value: false
lr_scheduler_type:
    value: cosine
max_seq_length:
    value: 2048
max_train_steps:
    value: 228
model_name_or_path:
    value: meta-llama/Llama-3.1-8B-Instruct
neftune_alpha:
    value: null
num_train_epochs:
    value: 6
output_dir:
    value: output/llama3-8b_im_sft
overwrite_cache:
    value: false
padding_side:
    value: null
per_device_train_batch_size:
    value: 1
preprocessing_num_workers:
    value: 16
report_to:
    value: wandb
resume_from_checkpoint:
    value: null
rope_scaling_factor:
    value: 2
rope_scaling_type:
    value: linear
run_name:
    value: llama3-8b-im
seed:
    value: null
tokenizer_name:
    value: meta-llama/Llama-3.1-8B-Instruct
train_file:
    value: data/silverpairs_prompt_completion.jsonl
use_8bit_optimizer:
    value: false
use_flash_attn:
    value: true
use_lm_loss:
    value: true
use_lm_modelling:
    value: false
use_lora:
    value: true
use_qlora:
    value: false
use_slow_tokenizer:
    value: false
warmup_ratio:
    value: 0.05
weight_decay:
    value: 0
with_tracking:
    value: true
