Namespace(dataset_name=None, dataset_config_name=None, train_file='data/silverpairs_prompt_completion.jsonl', model_name_or_path='meta-llama/Llama-3.1-8B-Instruct', config_name=None, use_lora=True, lora_rank=8, lora_alpha=32.0, lora_dropout=0.05, use_flash_attn=True, tokenizer_name='meta-llama/Llama-3.1-8B-Instruct', use_slow_tokenizer=False, max_seq_length=42000, per_device_train_batch_size=1, learning_rate=0.0001, weight_decay=0.0, num_train_epochs=6, max_train_steps=None, gradient_accumulation_steps=8, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, warmup_ratio=0.05, output_dir='output/llama3-8b_im_sft', seed=None, preprocessing_num_workers=16, overwrite_cache=False, checkpointing_steps='50', logging_steps=1, resume_from_checkpoint=None, with_tracking=True, report_to='wandb', low_cpu_mem_usage=False, gradient_checkpointing=False, use_qlora=False, clip_grad_norm=-1, use_8bit_optimizer=False, apply_kl_div_loss=False, kl_penalty_ctl=1.0, use_lm_loss=True, block_size=None, use_lm_modelling=False, padding_side=None, neftune_alpha=None, lora_target='q_proj,v_proj', run_name='llama3-8b-im', rope_scaling_type='linear', rope_scaling_factor=2.0)